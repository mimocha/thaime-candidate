{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464faa89",
   "metadata": {},
   "source": [
    "# LLM-based Thai-to-Latin Transliteration\n",
    "\n",
    "This notebook uses an OpenAI LLM to transliterate a list of common Thai words into various Latin script representations. The goal is to generate multiple possible transliterations that a human might use.\n",
    "\n",
    "The process is as follows:\n",
    "1. Load the most common Thai words from `artifacts/combined_word_freq.json`.\n",
    "2. Set parameters for `TOP_K` words to transliterate and the `BATCH_SIZE` for API calls.\n",
    "3. Prepare a system prompt to instruct the LLM on the transliteration task.\n",
    "4. Batch the words and send requests to the OpenAI API.\n",
    "5. Parse the JSON response from the LLM.\n",
    "6. Save the aggregated transliterations to `artifacts/llm_transliteration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b1175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- Parameters ---\n",
    "# Note: If the API key is not found in the .env file,\n",
    "# it will try to use the OPENAI_API_KEY environment variable.\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file or environment variables\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Select model\n",
    "MODEL = \"gpt-4.1-mini\"\n",
    "\n",
    "# Transliteration Parameters\n",
    "TOP_K = 10000  # Number of most common words to transliterate\n",
    "BATCH_SIZE = 50  # Number of words to send in each API call\n",
    "\n",
    "# Define paths\n",
    "WORD_FREQ_PATH = Path(\"../artifacts/combined_word_freq.json\")\n",
    "OUTPUT_DIR = Path(\"../artifacts/llm_transliteration\")\n",
    "OUTPUT_PATH = OUTPUT_DIR / f\"{MODEL}.json\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce06f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top 10000 words for transliteration.\n",
      "Batch size set to 50 words per API call.\n",
      "First 10 words: ['ที่', 'ไม่', 'ใน', 'มี', 'และ', 'ของ', 'ได้', 'เป็น', 'มา', 'ไป']\n",
      "Partial results file: ../artifacts/llm_transliteration/gpt-4.1-mini.json already exists, attempting resume...\n",
      "Read 500 entries\n",
      "Final output size: 10000 words\n",
      "Number of words to work on: 9500 words\n"
     ]
    }
   ],
   "source": [
    "# --- Load the combined word frequencies data ---\n",
    "with open(WORD_FREQ_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    word_freq = json.load(f)\n",
    "\n",
    "# Get the list of words to transliterate\n",
    "words_to_transliterate = list(word_freq.keys())[:TOP_K]\n",
    "\n",
    "print(f\"Selected top {TOP_K} words for transliteration.\")\n",
    "print(f\"Batch size set to {BATCH_SIZE} words per API call.\")\n",
    "print(f\"First 10 words: {words_to_transliterate[:10]}\")\n",
    "\n",
    "# Create or Load partially completed file\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    print(f\"Partial results file: {OUTPUT_PATH} already exists, attempting resume...\")\n",
    "    with open(OUTPUT_PATH, 'r') as fp:\n",
    "        results_dict = json.load(fp)\n",
    "        print(f\"Read {len(results_dict)} entries\")\n",
    "else:\n",
    "    print(f\"No partial results found, starting over at file {OUTPUT_PATH}\")\n",
    "    results_dict = dict()\n",
    "\n",
    "# Add any new words to result dictionary\n",
    "pending_words = list()\n",
    "for word in words_to_transliterate:\n",
    "    if (word not in results_dict):\n",
    "        results_dict[word] = list()\n",
    "        pending_words.append(word)\n",
    "    elif (word in results_dict) and (len(results_dict[word]) == 0):\n",
    "        pending_words.append(word)\n",
    "\n",
    "print(f\"Final output size: {len(results_dict)} words\")\n",
    "print(f\"Number of words to work on: {len(pending_words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3d1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function & System Prompt ---\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert in Thai to English transliteration. Your task is to provide multiple, common-sense transliterations for a given list of Thai words. The transliterations should reflect how a human user might type the word using a standard Latin keyboard.\n",
    "\n",
    "Please follow these guidelines:\n",
    "1. For each Thai word, provide a list of possible transliterations.\n",
    "2. The transliterations should be lowercase.\n",
    "3. Prioritize common and intuitive spellings over strict phonetic or academic systems.\n",
    "4. Provide as many reasonable variations as you can think of. For example, for \"สวัสดี\", you might provide [\"sawatdee\", \"sawasdee\", \"sawasdi\", \"sawatdi\"].\n",
    "5. The output MUST be a valid JSON object where keys are the original Thai words and values are lists of their transliterations.\n",
    "\n",
    "Example Input:\n",
    "[\"สวัสดี\", \"ขอบคุณ\", \"ส\"]\n",
    "\n",
    "Example Output:\n",
    "{\n",
    "  \"สวัสดี\": [\"sawatdee\", \"sawasdee\", \"sawasdi\", \"sawatdi\"],\n",
    "  \"ขอบคุณ\": [\"khobkhun\", \"khopkhun\", \"kobkun\", \"kopkun\"],\n",
    "  \"ส\": [\"s\", \"so\", \"sor\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def get_transliterations_from_llm(word_batch) -> dict | None:\n",
    "    \"\"\"\n",
    "    Sends a batch of words to the OpenAI API for transliteration.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(word_batch, ensure_ascii=False)}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.35,\n",
    "            timeout=30,\n",
    "            max_completion_tokens=3000\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        return json.loads(content)\n",
    "\n",
    "    except openai.APITimeoutError as e:\n",
    "        print(f\"Timeout error: {e}\")\n",
    "    except openai.APIError as e:\n",
    "        print(f\"OpenAI API Error: {e}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to decode JSON from response:\\n{content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    print(\"\\nResponse Dump:\\n\")\n",
    "    print(response)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f958b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLM transliteration for 9500 words in 190 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transliterating batches: 100%|██████████| 190/190 [1:02:00<00:00, 19.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished transliteration. 0 words remaining to work on.\n",
      "Transliteration dictionary size: 10000 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main Processing Loop ---\n",
    "\n",
    "# Add pending words to batches\n",
    "word_batches = [\n",
    "    pending_words[i : i+BATCH_SIZE]\n",
    "    for i in range(0, len(pending_words), BATCH_SIZE)\n",
    "]\n",
    "\n",
    "print(f\"Starting LLM transliteration for {len(pending_words)} words in {len(word_batches)} batches...\")\n",
    "\n",
    "for batch in tqdm(word_batches, desc=\"Transliterating batches\"):\n",
    "    result = get_transliterations_from_llm(batch)\n",
    "    if not result:\n",
    "        print(\"No results returned for batch\")\n",
    "        continue\n",
    "\n",
    "    for thai, latin in result.items():\n",
    "        # Hallucination catch\n",
    "        if thai not in pending_words:\n",
    "            print(f\"Hallucination Error: This word was not requested - {thai}\\n Results dump below:\\n\")\n",
    "            print(result)\n",
    "            continue\n",
    "        # Clean results - remove duplicates, trim, lowercase\n",
    "        clean_latin = list({token.strip(' -').lower() for token in latin})\n",
    "        # Empty list catch\n",
    "        if len(clean_latin) == 0:\n",
    "            continue\n",
    "        # Add results to output dict\n",
    "        results_dict[thai] = clean_latin\n",
    "        # Remove words with successful transliteration from pending list\n",
    "        pending_words.remove(thai)\n",
    "\n",
    "print(f\"\\nFinished transliteration. {len(pending_words)} words remaining to work on.\")\n",
    "print(f\"Transliteration dictionary size: {len(results_dict)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413723da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All words have at least one valid transliteration.\n"
     ]
    }
   ],
   "source": [
    "# --- Warn of remaining work ---\n",
    "\n",
    "if pending_words:\n",
    "    print(f\"\\nWarning: The following {len(pending_words)} words have no valid transliterations:\")\n",
    "    for word in pending_words:\n",
    "        print(f\"- {word}\")\n",
    "else:\n",
    "    print(\"\\nAll words have at least one valid transliteration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325b9427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of transliterations:\n",
      "- ที่: ['tee', 'thi']\n",
      "- ไม่: ['my', 'mai', 'mhai']\n",
      "- ใน: ['nai']\n",
      "- มี: ['mee', 'mi']\n",
      "- และ: ['lae', 'le']\n",
      "- ของ: ['kong', 'khong']\n",
      "- ได้: ['dai']\n",
      "- เป็น: ['pen', 'bpen']\n",
      "- มา: ['ma', 'maa']\n",
      "- ไป: ['bpai', 'pai']\n"
     ]
    }
   ],
   "source": [
    "# --- Display Sample Results ---\n",
    "\n",
    "print(\"Sample of transliterations:\")\n",
    "sample_count = 0\n",
    "for word, transliterations in results_dict.items():\n",
    "    if sample_count < 10:\n",
    "        print(f\"- {word}: {transliterations}\")\n",
    "        sample_count += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c5c2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved all transliterations to: ../artifacts/llm_transliteration/gpt-4.1-mini.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save Results to File ---\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nSaved all transliterations to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98eeca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thaime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
